{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_precision</th>\n",
       "      <th>ndcg_at_10</th>\n",
       "      <th>n_steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pvdm_original_articles_1_300_300_cosine</th>\n",
       "      <td>0.203961</td>\n",
       "      <td>0.228228</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pvdm_original_articles_5_300_300_cosine</th>\n",
       "      <td>0.926748</td>\n",
       "      <td>0.932935</td>\n",
       "      <td>5155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         average_precision  ndcg_at_10 n_steps\n",
       "pvdm_original_articles_1_300_300_cosine           0.203961    0.228228    1031\n",
       "pvdm_original_articles_5_300_300_cosine           0.926748    0.932935    5155"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_folders = glob.glob('experiments/*/')\n",
    "params = {}\n",
    "results = {}\n",
    "for exp_folder in exp_folders:\n",
    "    exp_name = exp_folder.split(os.sep)[1]\n",
    "    if not os.path.exists(os.path.join(exp_folder, 'params.p')):\n",
    "        continue\n",
    "    with open(os.path.join(exp_folder, 'params.p'), 'rb') as params_file:\n",
    "        params[exp_name] = pickle.load(params_file)\n",
    "    with open(os.path.join(exp_folder, 'results.p'), 'rb') as results_file:\n",
    "        results[exp_name] = pickle.load(results_file)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "params_df = pd.DataFrame()\n",
    "for exp, result in results.items():\n",
    "    mean_result = pd.DataFrame.from_dict(result).mean()\n",
    "    mean_result.name=exp\n",
    "    result_df = result_df.append(mean_result)\n",
    "result_df = pd.concat([result_df, pd.DataFrame.from_dict(params).T], axis=1)\n",
    "result_df[['average_precision', 'ndcg_at_10', 'n_steps']].sort_values('n_steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_precision</th>\n",
       "      <th>ndcg_at_10</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>architecture</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>concat</th>\n",
       "      <th>data</th>\n",
       "      <th>dist_measure</th>\n",
       "      <th>document_size</th>\n",
       "      <th>emb_size_d</th>\n",
       "      <th>...</th>\n",
       "      <th>iterations</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss_type</th>\n",
       "      <th>n_neg_samples</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>nsteps</th>\n",
       "      <th>optimize</th>\n",
       "      <th>prior_sample_size</th>\n",
       "      <th>vocabulary_size</th>\n",
       "      <th>window_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc2vec-gensim_aminer_org_v1_5_300_300_cosine</th>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>doc2vec-gensim</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>aminer_org_v1</td>\n",
       "      <td>cosine</td>\n",
       "      <td>111000</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sampled_softmax_loss</td>\n",
       "      <td>64</td>\n",
       "      <td>555000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2vec-gensim_original_articles_100001_300_300_cosine</th>\n",
       "      <td>0.677843</td>\n",
       "      <td>0.687516</td>\n",
       "      <td>doc2vec-gensim</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>original_articles</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1031</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>sampled_softmax_loss</td>\n",
       "      <td>64</td>\n",
       "      <td>100001</td>\n",
       "      <td>100001</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2vec-gensim_original_articles_5_100_100_cosine</th>\n",
       "      <td>0.675223</td>\n",
       "      <td>0.680724</td>\n",
       "      <td>doc2vec-gensim</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>original_articles</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1031</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sampled_softmax_loss</td>\n",
       "      <td>64</td>\n",
       "      <td>5155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2vec-gensim_original_articles_5_300_300_cosine</th>\n",
       "      <td>0.673040</td>\n",
       "      <td>0.682589</td>\n",
       "      <td>doc2vec-gensim</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>original_articles</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1031</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sampled_softmax_loss</td>\n",
       "      <td>64</td>\n",
       "      <td>5155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pvdm_original_articles_10000_300_300_cosine</th>\n",
       "      <td>0.649623</td>\n",
       "      <td>0.670708</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>original_articles</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1031</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>sampled_softmax_loss</td>\n",
       "      <td>64</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>50000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pvdm_original_articles_1000_300_300_cosine</th>\n",
       "      <td>0.674012</td>\n",
       "      <td>0.685618</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>original_articles</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1031</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>sampled_softmax_loss</td>\n",
       "      <td>64</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>50000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pvdm_original_articles_100_300_300_cosine</th>\n",
       "      <td>0.679642</td>\n",
       "      <td>0.689899</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>original_articles</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1031</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>sampled_softmax_loss</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>50000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pvdm_original_articles_50000_300_300_cosine</th>\n",
       "      <td>0.639258</td>\n",
       "      <td>0.665104</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>pvdm</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>original_articles</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1031</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>sampled_softmax_loss</td>\n",
       "      <td>64</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>50000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    average_precision  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine                0.000702   \n",
       "doc2vec-gensim_original_articles_100001_300_300...           0.677843   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine            0.675223   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine            0.673040   \n",
       "pvdm_original_articles_10000_300_300_cosine                  0.649623   \n",
       "pvdm_original_articles_1000_300_300_cosine                   0.674012   \n",
       "pvdm_original_articles_100_300_300_cosine                    0.679642   \n",
       "pvdm_original_articles_50000_300_300_cosine                  0.639258   \n",
       "\n",
       "                                                    ndcg_at_10  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine         0.001029   \n",
       "doc2vec-gensim_original_articles_100001_300_300...    0.687516   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine     0.680724   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine     0.682589   \n",
       "pvdm_original_articles_10000_300_300_cosine           0.670708   \n",
       "pvdm_original_articles_1000_300_300_cosine            0.685618   \n",
       "pvdm_original_articles_100_300_300_cosine             0.689899   \n",
       "pvdm_original_articles_50000_300_300_cosine           0.665104   \n",
       "\n",
       "                                                         algorithm  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine       doc2vec-gensim   \n",
       "doc2vec-gensim_original_articles_100001_300_300...  doc2vec-gensim   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine   doc2vec-gensim   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine   doc2vec-gensim   \n",
       "pvdm_original_articles_10000_300_300_cosine                   pvdm   \n",
       "pvdm_original_articles_1000_300_300_cosine                    pvdm   \n",
       "pvdm_original_articles_100_300_300_cosine                     pvdm   \n",
       "pvdm_original_articles_50000_300_300_cosine                   pvdm   \n",
       "\n",
       "                                                   architecture batch_size  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine              pvdm        128   \n",
       "doc2vec-gensim_original_articles_100001_300_300...         pvdm        128   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine          pvdm        128   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine          pvdm        128   \n",
       "pvdm_original_articles_10000_300_300_cosine                pvdm        128   \n",
       "pvdm_original_articles_1000_300_300_cosine                 pvdm        128   \n",
       "pvdm_original_articles_100_300_300_cosine                  pvdm        128   \n",
       "pvdm_original_articles_50000_300_300_cosine                pvdm        128   \n",
       "\n",
       "                                                   concat               data  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine        True      aminer_org_v1   \n",
       "doc2vec-gensim_original_articles_100001_300_300...   True  original_articles   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine    True  original_articles   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine    True  original_articles   \n",
       "pvdm_original_articles_10000_300_300_cosine          True  original_articles   \n",
       "pvdm_original_articles_1000_300_300_cosine           True  original_articles   \n",
       "pvdm_original_articles_100_300_300_cosine            True  original_articles   \n",
       "pvdm_original_articles_50000_300_300_cosine          True  original_articles   \n",
       "\n",
       "                                                   dist_measure document_size  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine            cosine        111000   \n",
       "doc2vec-gensim_original_articles_100001_300_300...       cosine          1031   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine        cosine          1031   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine        cosine          1031   \n",
       "pvdm_original_articles_10000_300_300_cosine              cosine          1031   \n",
       "pvdm_original_articles_1000_300_300_cosine               cosine          1031   \n",
       "pvdm_original_articles_100_300_300_cosine                cosine          1031   \n",
       "pvdm_original_articles_50000_300_300_cosine              cosine          1031   \n",
       "\n",
       "                                                   emb_size_d     ...      \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine             300     ...       \n",
       "doc2vec-gensim_original_articles_100001_300_300...        300     ...       \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine         100     ...       \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine         300     ...       \n",
       "pvdm_original_articles_10000_300_300_cosine               300     ...       \n",
       "pvdm_original_articles_1000_300_300_cosine                300     ...       \n",
       "pvdm_original_articles_100_300_300_cosine                 300     ...       \n",
       "pvdm_original_articles_50000_300_300_cosine               300     ...       \n",
       "\n",
       "                                                   iterations learning_rate  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine               5             1   \n",
       "doc2vec-gensim_original_articles_100001_300_300...        NaN             1   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine           5             1   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine           5             1   \n",
       "pvdm_original_articles_10000_300_300_cosine               NaN             1   \n",
       "pvdm_original_articles_1000_300_300_cosine                NaN             1   \n",
       "pvdm_original_articles_100_300_300_cosine                 NaN             1   \n",
       "pvdm_original_articles_50000_300_300_cosine               NaN             1   \n",
       "\n",
       "                                                               loss_type  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine       sampled_softmax_loss   \n",
       "doc2vec-gensim_original_articles_100001_300_300...  sampled_softmax_loss   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine   sampled_softmax_loss   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine   sampled_softmax_loss   \n",
       "pvdm_original_articles_10000_300_300_cosine         sampled_softmax_loss   \n",
       "pvdm_original_articles_1000_300_300_cosine          sampled_softmax_loss   \n",
       "pvdm_original_articles_100_300_300_cosine           sampled_softmax_loss   \n",
       "pvdm_original_articles_50000_300_300_cosine         sampled_softmax_loss   \n",
       "\n",
       "                                                   n_neg_samples n_steps  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine                 64  555000   \n",
       "doc2vec-gensim_original_articles_100001_300_300...            64  100001   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine             64    5155   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine             64    5155   \n",
       "pvdm_original_articles_10000_300_300_cosine                   64   10000   \n",
       "pvdm_original_articles_1000_300_300_cosine                    64    1000   \n",
       "pvdm_original_articles_100_300_300_cosine                     64     100   \n",
       "pvdm_original_articles_50000_300_300_cosine                   64   50000   \n",
       "\n",
       "                                                    nsteps optimize  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine          NaN  Adagrad   \n",
       "doc2vec-gensim_original_articles_100001_300_300...  100001  Adagrad   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine      NaN  Adagrad   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine      NaN  Adagrad   \n",
       "pvdm_original_articles_10000_300_300_cosine          10000  Adagrad   \n",
       "pvdm_original_articles_1000_300_300_cosine            1000  Adagrad   \n",
       "pvdm_original_articles_100_300_300_cosine              100  Adagrad   \n",
       "pvdm_original_articles_50000_300_300_cosine          50000  Adagrad   \n",
       "\n",
       "                                                   prior_sample_size  \\\n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine                    NaN   \n",
       "doc2vec-gensim_original_articles_100001_300_300...               NaN   \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine                NaN   \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine                NaN   \n",
       "pvdm_original_articles_10000_300_300_cosine                       10   \n",
       "pvdm_original_articles_1000_300_300_cosine                        10   \n",
       "pvdm_original_articles_100_300_300_cosine                         10   \n",
       "pvdm_original_articles_50000_300_300_cosine                       10   \n",
       "\n",
       "                                                   vocabulary_size window_size  \n",
       "doc2vec-gensim_aminer_org_v1_5_300_300_cosine                50000           8  \n",
       "doc2vec-gensim_original_articles_100001_300_300...           50000           8  \n",
       "doc2vec-gensim_original_articles_5_100_100_cosine            50000           8  \n",
       "doc2vec-gensim_original_articles_5_300_300_cosine            50000           8  \n",
       "pvdm_original_articles_10000_300_300_cosine                  50000           8  \n",
       "pvdm_original_articles_1000_300_300_cosine                   50000           8  \n",
       "pvdm_original_articles_100_300_300_cosine                    50000           8  \n",
       "pvdm_original_articles_50000_300_300_cosine                  50000           8  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "raw_data_folder = 'aminer_org_v1'\n",
    "rel_labels_fname = 'relevance_labels_' + raw_data_folder + '.p'\n",
    "with open(rel_labels_fname, 'rb') as rel_lab_file:\n",
    "    _, _, _, tokenized, _, sorted_bm25_indices = pickle.load(rel_lab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031\n",
      "Computing BM25...\n",
      "Done computing bm25, compute average IDF...\n",
      "Done computing average IDF.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "output_fname='rel_labels.p'\n",
    "folder='original_articles'\n",
    "source_dict = {}  # maps article filename to source\n",
    "docs = []  # list with documents\n",
    "doc_names = []  # doc names with same index as docs\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), 'raw_data.tmp', folder)\n",
    "for subdir, dirs, files in os.walk(data_path):\n",
    "    files = [fi for fi in files if fi.endswith(\".txt\")]\n",
    "    for file in files:\n",
    "        path = os.path.join(subdir, file)\n",
    "        folder_name = subdir.split(os.path.sep)[-1]\n",
    "        fname = file[:-4]\n",
    "        source_dict[fname] = subdir.split(os.path.sep)[-1]\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            docs.append(f.read())\n",
    "        doc_names.append(fname)\n",
    "\n",
    "tokenized = []\n",
    "for doc in docs:\n",
    "    tokens = [word for sent in nltk.sent_tokenize(doc) for word in nltk.word_tokenize(sent)]\n",
    "    tokenized.append(tokens)\n",
    "\n",
    "print(len(docs))\n",
    "print(\"Computing BM25...\")\n",
    "bm25 = gensim.summarization.bm25.BM25(tokenized)\n",
    "print(\"Done computing bm25, compute average IDF...\")\n",
    "average_idf = sum(map(lambda k: float(bm25.idf[k]), bm25.idf.keys())) / len(bm25.idf.keys())\n",
    "print(\"Done computing average IDF.\")\n",
    "bm25_scores = []\n",
    "sorted_bm25_indices = []\n",
    "len_tokenized = len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 517 Definition of eligible capital By 31 December 2014 the Commission shall review and report on the appropriateness of the definition of eligible capital being applied for the purposes of Title III of Part Two and Part Four and shall submit that report to the European Parliament and the Council , and , if appropriate , a legislative proposal .\n",
      "867\n"
     ]
    }
   ],
   "source": [
    "doc = tokenized[500]\n",
    "print(' '.join(doc))\n",
    "temp_bm25_scores = bm25.get_scores(doc, average_idf)\n",
    "temp_bm25_scores = temp_bm25_scores\n",
    "sorted_indices = sorted(range(len(temp_bm25_scores)), key=lambda x: temp_bm25_scores[x], reverse=True)\n",
    "# print(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "class BM25 :\n",
    "    def __init__(self, fn_docs) :\n",
    "        self.dictionary = corpora.Dictionary()\n",
    "        self.doc_names = []\n",
    "        self.source_dict = {}\n",
    "        self.DF = {}\n",
    "        self.DocTF = []\n",
    "        self.DocIDF = {}\n",
    "        self.N = 0\n",
    "        self.DocAvgLen = 0\n",
    "        self.fn_docs = fn_docs\n",
    "        self.DocLen = []\n",
    "        self.buildDictionary()\n",
    "        self.TFIDF_Generator()\n",
    "\n",
    "    def buildDictionary(self) :\n",
    "        raw_data = []\n",
    "#         for line in file(self.fn_docs) :\n",
    "#             raw_data.append(line.strip().split(self.delimiter))\n",
    "        data_path = os.path.join(os.getcwd(), 'raw_data.tmp', self.fn_docs)\n",
    "        for subdir, dirs, files in os.walk(data_path):\n",
    "            files = [fi for fi in files if fi.endswith(\".txt\")]\n",
    "            for file in files:\n",
    "                path = os.path.join(subdir, file)\n",
    "                folder_name = subdir.split(os.path.sep)[-1]\n",
    "                fname = file[:-4]\n",
    "                self.source_dict[fname] = subdir.split(os.path.sep)[-1]\n",
    "                with open(path, 'r', encoding='utf8') as f:\n",
    "                    raw_data.append([word for sent in nltk.sent_tokenize(f.read()) for word in nltk.word_tokenize(sent)])\n",
    "                self.doc_names.append(fname)\n",
    "        self.dictionary.add_documents(raw_data)\n",
    "\n",
    "    def TFIDF_Generator(self, base=math.e) :\n",
    "        docTotalLen = 0\n",
    "        data_path = os.path.join(os.getcwd(), 'raw_data.tmp', self.fn_docs)\n",
    "        for subdir, dirs, files in os.walk(data_path):\n",
    "            files = [fi for fi in files if fi.endswith(\".txt\")]\n",
    "            for file in files:\n",
    "                path = os.path.join(subdir, file)\n",
    "                with open(path, 'r', encoding='utf8') as f:\n",
    "                    doc = [word for sent in nltk.sent_tokenize(f.read()) for word in nltk.word_tokenize(sent)]\n",
    "                docTotalLen += len(doc)\n",
    "                self.DocLen.append(len(doc))\n",
    "                bow = dict([(term, freq*1.0/len(doc)) for term, freq in self.dictionary.doc2bow(doc)])\n",
    "                for term, tf in bow.items() :\n",
    "                    if term not in self.DF :\n",
    "                        self.DF[term] = 0\n",
    "                    self.DF[term] += 1\n",
    "                self.DocTF.append(bow)\n",
    "                self.N = self.N + 1\n",
    "        for term in self.DF:\n",
    "            self.DocIDF[term] = math.log((self.N - self.DF[term] +0.5) / (self.DF[term] + 0.5), base)\n",
    "        self.DocAvgLen = docTotalLen / self.N\n",
    "\n",
    "    def BM25Score(self, Query=[], k1=1.5, b=0.75) :\n",
    "        query_bow = self.dictionary.doc2bow(Query)\n",
    "        scores = []\n",
    "        for idx, doc in enumerate(self.DocTF) :\n",
    "            commonTerms = set(dict(query_bow).keys()) & set(doc.keys())\n",
    "            tmp_score = []\n",
    "            doc_terms_len = self.DocLen[idx]\n",
    "            for term in commonTerms :\n",
    "                upper = (doc[term] * (k1+1))\n",
    "                below = ((doc[term]) + k1*(1 - b + b*doc_terms_len/self.DocAvgLen))\n",
    "                tmp_score.append(self.DocIDF[term] * upper / below)\n",
    "            scores.append(sum(tmp_score))\n",
    "        return scores\n",
    "\n",
    "    def TFIDF(self) :\n",
    "        tfidf = []\n",
    "        for doc in self.DocTF :\n",
    "            doc_tfidf  = [(term, tf*self.DocIDF[term]) for term, tf in doc.items()]\n",
    "            doc_tfidf.sort()\n",
    "            tfidf.append(doc_tfidf)\n",
    "        return tfidf\n",
    "\n",
    "    def Items(self) :\n",
    "        # Return a list [(term_idx, term_desc),]\n",
    "        items = self.dictionary.items()\n",
    "        items.sort()\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "output_fname='rel_labels.p'\n",
    "folder='aminer_org_v1'\n",
    "source_dict = {}  # maps article filename to source\n",
    "docs = []  # list with documents\n",
    "doc_names = []  # doc names with same index as docs\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), 'raw_data.tmp', folder)\n",
    "for subdir, dirs, files in os.walk(data_path):\n",
    "    files = [fi for fi in files if fi.endswith(\".txt\")]\n",
    "    for file in files:\n",
    "        path = os.path.join(subdir, file)\n",
    "        folder_name = subdir.split(os.path.sep)[-1]\n",
    "        fname = file[:-4]\n",
    "        source_dict[fname] = subdir.split(os.path.sep)[-1]\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            docs.append(f.read())\n",
    "        doc_names.append(fname)\n",
    "\n",
    "tokenized = []\n",
    "for doc in docs:\n",
    "    tokens = [word for sent in nltk.sent_tokenize(doc) for word in nltk.word_tokenize(sent)]\n",
    "    tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_docs = 'aminer_org_v1'\n",
    "bm25 = BM25(fn_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91283\n",
      "[91283, 6339, 72175, 33383, 57814, 109712, 19367, 94624, 35338, 22166]\n",
      "\n",
      "Original text:\n",
      "\n",
      "Dynamically managing the communication-parallelism trade-off in future clustered processors Clustered microarchitectures are an attractive alternative to large monolithic superscalar designs due to their potential for higher clock rates in the face of increasingly wire-delay-constrained process technologies . As increasing transistor counts allow an increase in the number of clusters , thereby allowing more aggressive use of instruction-level parallelism ( ILP ) , the inter-cluster communication increases as data values get spread across a wider area . As a result of the emergence of this trade-off between communication and parallelism , a subset of the total on-chip clusters is optimal for performance . To match the hardware to the application 's needs , we use a robust algorithm to dynamically tune the clustered architecture . The algorithm , which is based on program metrics gathered at periodic intervals , achieves an 11 % performance improvement on average over the best statically defined architecture . We also show that the use of additional hardware and reconfiguration at basic block boundaries can achieve average improvements of 15 % . Our results demonstrate that reconfiguration provides an effective solution to the communication and parallelism trade-off inherent in the communication-bound processors of the future .\n",
      "\n",
      "Top 5:\n",
      "\n",
      "Dynamically managing the communication-parallelism trade-off in future clustered processors Clustered microarchitectures are an attractive alternative to large monolithic superscalar designs due to their potential for higher clock rates in the face of increasingly wire-delay-constrained process technologies . As increasing transistor counts allow an increase in the number of clusters , thereby allowing more aggressive use of instruction-level parallelism ( ILP ) , the inter-cluster communication increases as data values get spread across a wider area . As a result of the emergence of this trade-off between communication and parallelism , a subset of the total on-chip clusters is optimal for performance . To match the hardware to the application 's needs , we use a robust algorithm to dynamically tune the clustered architecture . The algorithm , which is based on program metrics gathered at periodic intervals , achieves an 11 % performance improvement on average over the best statically defined architecture . We also show that the use of additional hardware and reconfiguration at basic block boundaries can achieve average improvements of 15 % . Our results demonstrate that reconfiguration provides an effective solution to the communication and parallelism trade-off inherent in the communication-bound processors of the future . \n",
      "--------------------\n",
      "\n",
      "Emergent process design No abstract available \n",
      "--------------------\n",
      "\n",
      "Guest Editors ' Introduction : High-Performance Reconfigurable Computing High-performance reconfigurable computers have the potential to exploit coarse-grained functional parallelism as well as fine-grained instruction-level parallelism through direct hardware execution on FPGAs . \n",
      "--------------------\n",
      "\n",
      "Agile software reuse recommender No abstract available \n",
      "--------------------\n",
      "\n",
      "Self-assessment procedure XVII A self-assessment procedure dealing with ACM \n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inspect(index):\n",
    "    print(index)\n",
    "    Query = tokenized[index]\n",
    "    scores = bm25.BM25Score(Query)\n",
    "    sorted_indices = sorted(range(len(scores)), key=lambda x: scores[x], reverse=True)\n",
    "    print(sorted_indices[:10])\n",
    "    print('\\nOriginal text:\\n')\n",
    "    print(' '.join(tokenized[index][:300]))\n",
    "    print('\\nTop 5:\\n')\n",
    "    for i in range(5):\n",
    "        print(' '.join(tokenized[sorted_indices[i]]),'\\n--------------------\\n')\n",
    "\n",
    "index = np.random.randint(len(tokenized))\n",
    "inspect(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexandervansomeren/Documents/Studie/Msc_AI/Thesis/regulatory tracker/doc2vec_pipeline/raw_data.tmp/original_articles\n"
     ]
    }
   ],
   "source": [
    "fn_docs = 'original_articles'\n",
    "\n",
    "\n",
    "raw_data = []\n",
    "data_path = os.path.join(os.getcwd(), 'raw_data.tmp', fn_docs)\n",
    "print(data_path)\n",
    "doc_names=[]\n",
    "dictionary=corpora.Dictionary()\n",
    "for subdir, dirs, files in os.walk(data_path):\n",
    "    files = [fi for fi in files if fi.endswith(\".txt\")]\n",
    "    for file in files:\n",
    "        path = os.path.join(subdir, file)\n",
    "        folder_name = subdir.split(os.path.sep)[-1]\n",
    "        fname = file[:-4]\n",
    "        source_dict[fname] = subdir.split(os.path.sep)[-1]\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            raw_data.append([word for sent in nltk.sent_tokenize(f.read()) for word in nltk.word_tokenize(sent)])\n",
    "        doc_names.append(fname)\n",
    "dictionary.add_documents(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow() missing 1 required positional argument: 'document'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-5f98ec8bc765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow() missing 1 required positional argument: 'document'"
     ]
    }
   ],
   "source": [
    "dictionary.doc2bow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:regulatory_tracker]",
   "language": "python",
   "name": "conda-env-regulatory_tracker-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nbpresent": {
   "slides": {
    "7bdfc0fc-9cb9-4774-aaa5-ec1059c174ed": {
     "id": "7bdfc0fc-9cb9-4774-aaa5-ec1059c174ed",
     "prev": "8b4c6ddd-df51-456d-b79a-2ca76c61163f",
     "regions": {
      "b4df9567-5fa8-45a1-b5fc-1330bfafb954": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4f833753-0a8c-48d8-b5f1-6b8660996bb6",
        "part": "whole"
       },
       "id": "b4df9567-5fa8-45a1-b5fc-1330bfafb954"
      }
     }
    },
    "8b4c6ddd-df51-456d-b79a-2ca76c61163f": {
     "id": "8b4c6ddd-df51-456d-b79a-2ca76c61163f",
     "prev": null,
     "regions": {}
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
